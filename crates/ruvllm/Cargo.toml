[package]
name = "ruvllm-integration"
version.workspace = true
edition.workspace = true
rust-version.workspace = true
license.workspace = true
authors.workspace = true
repository.workspace = true
description = "LLM serving runtime with Ruvector integration - Paged attention, KV cache, and SONA learning"

[dependencies]
# Ruvector integration
ruvector-core = { path = "../ruvector-core", default-features = false, features = ["storage"] }
ruvector-sona = { path = "../sona", default-features = false, features = ["serde-support"] }

# Serialization
serde = { workspace = true }
serde_json = { workspace = true }

# Error handling
thiserror = { workspace = true }
anyhow = { workspace = true }
tracing = { workspace = true }

# Performance
dashmap = { workspace = true }
parking_lot = { workspace = true }
once_cell = { workspace = true }

# Time and UUID
chrono = { workspace = true, features = ["serde"] }
uuid = { workspace = true, features = ["v4", "serde"] }

# Math
ndarray = { workspace = true }
rand = { workspace = true }

# Serialization (binary)
bincode = "1.3"

# Async (optional for non-WASM)
tokio = { workspace = true, optional = true }

# Candle ML framework (optional)
candle-core = { version = "0.8", optional = true }
candle-nn = { version = "0.8", optional = true }
candle-transformers = { version = "0.8", optional = true }

# Tokenizers
tokenizers = { version = "0.20", optional = true, default-features = false, features = ["onig"] }

# HuggingFace Hub for model downloads
hf-hub = { version = "0.3", optional = true, features = ["tokio"] }

# Directories for cache
dirs = "5.0"

[dev-dependencies]
criterion = { workspace = true }
tempfile = "3.13"
tracing-subscriber = { workspace = true }

[features]
default = ["async-runtime"]
async-runtime = ["tokio"]
wasm = []

# Candle backend for LLM inference (Rust-native, Metal acceleration on Mac)
candle = ["candle-core", "candle-nn", "candle-transformers", "tokenizers", "hf-hub"]

# Metal acceleration for Apple Silicon (M1/M2/M3/M4)
metal = ["candle-core/metal", "candle-nn/metal", "candle-transformers/metal"]

# CUDA acceleration for NVIDIA GPUs
cuda = ["candle-core/cuda", "candle-nn/cuda", "candle-transformers/cuda"]

# Full inference backend with Metal (recommended for Mac)
inference-metal = ["candle", "metal"]

# Full inference backend with CUDA (recommended for NVIDIA)
inference-cuda = ["candle", "cuda"]

[lib]
crate-type = ["rlib"]

# Benchmark configurations
[[bench]]
name = "attention_bench"
harness = false

[[bench]]
name = "rope_bench"
harness = false

[[bench]]
name = "norm_bench"
harness = false

[[bench]]
name = "matmul_bench"
harness = false

[[bench]]
name = "lora_bench"
harness = false

[[bench]]
name = "e2e_bench"
harness = false
