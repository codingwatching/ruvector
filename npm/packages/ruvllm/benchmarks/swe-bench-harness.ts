/**
 * SWE-Bench Benchmark Harness for @ruvector/ruvllm
 *
 * Tests multiple small models on SWE-bench-style tasks with anti-overfitting measures:
 * - Stratified train/validation/test splits (60/20/20)
 * - K-fold cross-validation with statistical significance
 * - Multiple independent evaluation metrics
 * - Bootstrap confidence intervals
 * - Holdout set for final evaluation
 *
 * December 2025 - Small Model Focus (<7B parameters where possible)
 *
 * @requires Environment variables:
 *   - OPENAI_API_KEY (for GPT-4.1-nano, GPT-4.1-mini, GPT-5-nano)
 *   - ANTHROPIC_API_KEY (for Claude 3 Haiku)
 *   - OPENROUTER_API_KEY (for various open models)
 *   - DEEPSEEK_API_KEY (for DeepSeek Coder)
 */

import { performance } from 'perf_hooks';
import * as fs from 'fs/promises';
import * as path from 'path';
import * as crypto from 'crypto';

// ============================================================================
// Types & Interfaces
// ============================================================================

interface SmallModelConfig {
  name: string;
  provider: 'openai' | 'anthropic' | 'openrouter' | 'deepseek' | 'local';
  modelId: string;
  apiKey?: string;
  maxTokens: number;
  parameterCount: string; // e.g., "1.5B", "7B", "24B"
  costPer1kTokens: { input: number; output: number };
  temperature?: number;
  description: string;
}

interface SWEBenchTask {
  id: string;
  repo: string;
  issue: string;
  hint?: string;
  baseCommit: string;
  testPatch: string;
  difficulty: 'easy' | 'medium' | 'hard';
  category: 'bug_fix' | 'feature' | 'refactor' | 'test';
  expectedChanges: string[];
  holdout?: boolean; // For final evaluation only
}

interface TaskResult {
  taskId: string;
  modelName: string;
  resolved: boolean;
  partiallyResolved: boolean;
  generatedPatch: string;
  testsPassRate: number;
  latencyMs: number;
  tokensUsed: { inputTokens: number; outputTokens: number };
  cost: number;
  reasoning?: string;
  error?: string;
}

interface ModelBenchmarkMetrics {
  modelName: string;
  parameterCount: string;

  // Primary metrics
  resolveRate: number;          // % of tasks fully resolved
  partialResolveRate: number;   // % partially resolved
  testPassRate: number;         // Average tests passing

  // Anti-overfitting metrics
  trainResolveRate: number;     // Train set performance
  validationResolveRate: number; // Validation set performance
  testResolveRate: number;      // Test set performance
  overfitGap: number;           // Train - Test gap (lower is better)

  // Cross-validation metrics
  cvMean: number;               // K-fold CV mean
  cvStd: number;                // K-fold CV standard deviation
  cvConfidenceInterval: [number, number]; // 95% CI

  // Performance metrics
  avgLatencyMs: number;
  p95LatencyMs: number;
  throughputTasksPerMin: number;

  // Cost metrics
  totalCost: number;
  costPerResolvedTask: number;
  costEfficiencyScore: number;  // resolveRate / cost

  // Statistical significance
  statisticallyBetterThan: string[]; // Models this model significantly outperforms
  pValue?: number;
}

interface BenchmarkReport {
  timestamp: string;
  totalTasks: number;
  totalModels: number;
  kFolds: number;

  rankings: {
    byResolveRate: string[];
    byCostEfficiency: string[];
    byLatency: string[];
    byOverfitResistance: string[];
    overall: string[];
  };

  modelMetrics: ModelBenchmarkMetrics[];

  bestSmallModel: {
    name: string;
    reason: string;
    metrics: ModelBenchmarkMetrics;
  };

  antiOverfittingAnalysis: {
    averageOverfitGap: number;
    modelsWithSignificantOverfit: string[];
    recommendations: string[];
  };
}

// ============================================================================
// Anti-Overfitting Utilities
// ============================================================================

/**
 * Shuffle array with seed for reproducibility
 */
function seededShuffle<T>(array: T[], seed: string): T[] {
  const result = [...array];
  let hash = 0;
  for (let i = 0; i < seed.length; i++) {
    hash = ((hash << 5) - hash) + seed.charCodeAt(i);
    hash = hash & hash;
  }

  for (let i = result.length - 1; i > 0; i--) {
    hash = (hash * 1103515245 + 12345) & 0x7fffffff;
    const j = hash % (i + 1);
    [result[i], result[j]] = [result[j], result[i]];
  }
  return result;
}

/**
 * Stratified split maintaining category/difficulty distribution
 */
function stratifiedSplit(
  tasks: SWEBenchTask[],
  trainRatio: number,
  validRatio: number,
  seed: string
): { train: SWEBenchTask[]; valid: SWEBenchTask[]; test: SWEBenchTask[] } {
  // Group by difficulty and category
  const groups = new Map<string, SWEBenchTask[]>();
  for (const task of tasks) {
    const key = `${task.difficulty}_${task.category}`;
    if (!groups.has(key)) groups.set(key, []);
    groups.get(key)!.push(task);
  }

  const train: SWEBenchTask[] = [];
  const valid: SWEBenchTask[] = [];
  const test: SWEBenchTask[] = [];

  for (const [key, group] of groups) {
    const shuffled = seededShuffle(group, `${seed}_${key}`);
    const trainEnd = Math.floor(shuffled.length * trainRatio);
    const validEnd = trainEnd + Math.floor(shuffled.length * validRatio);

    train.push(...shuffled.slice(0, trainEnd));
    valid.push(...shuffled.slice(trainEnd, validEnd));
    test.push(...shuffled.slice(validEnd));
  }

  return { train, valid, test };
}

/**
 * K-fold cross-validation generator
 */
function* kFoldGenerator(tasks: SWEBenchTask[], k: number, seed: string) {
  const shuffled = seededShuffle(tasks, seed);
  const foldSize = Math.ceil(shuffled.length / k);

  for (let i = 0; i < k; i++) {
    const testStart = i * foldSize;
    const testEnd = Math.min((i + 1) * foldSize, shuffled.length);

    const testSet = shuffled.slice(testStart, testEnd);
    const trainSet = [...shuffled.slice(0, testStart), ...shuffled.slice(testEnd)];

    yield { fold: i + 1, trainSet, testSet };
  }
}

/**
 * Bootstrap confidence interval
 */
function bootstrapCI(values: number[], nBootstrap: number = 1000, ci: number = 0.95): [number, number] {
  const bootstrapMeans: number[] = [];

  for (let i = 0; i < nBootstrap; i++) {
    const sample = Array.from({ length: values.length }, () =>
      values[Math.floor(Math.random() * values.length)]
    );
    bootstrapMeans.push(sample.reduce((a, b) => a + b, 0) / sample.length);
  }

  bootstrapMeans.sort((a, b) => a - b);
  const alpha = (1 - ci) / 2;
  const lowerIdx = Math.floor(alpha * nBootstrap);
  const upperIdx = Math.floor((1 - alpha) * nBootstrap);

  return [bootstrapMeans[lowerIdx], bootstrapMeans[upperIdx]];
}

/**
 * Two-sample t-test for statistical significance
 */
function tTest(sample1: number[], sample2: number[]): { pValue: number; significant: boolean } {
  const n1 = sample1.length;
  const n2 = sample2.length;
  const mean1 = sample1.reduce((a, b) => a + b, 0) / n1;
  const mean2 = sample2.reduce((a, b) => a + b, 0) / n2;

  const var1 = sample1.reduce((sum, x) => sum + (x - mean1) ** 2, 0) / (n1 - 1);
  const var2 = sample2.reduce((sum, x) => sum + (x - mean2) ** 2, 0) / (n2 - 1);

  const pooledSE = Math.sqrt(var1 / n1 + var2 / n2);
  const tStat = Math.abs(mean1 - mean2) / pooledSE;

  // Approximate p-value using normal distribution for large samples
  const pValue = 2 * (1 - normalCDF(tStat));

  return { pValue, significant: pValue < 0.05 };
}

function normalCDF(x: number): number {
  const a1 =  0.254829592;
  const a2 = -0.284496736;
  const a3 =  1.421413741;
  const a4 = -1.453152027;
  const a5 =  1.061405429;
  const p  =  0.3275911;

  const sign = x < 0 ? -1 : 1;
  x = Math.abs(x) / Math.SQRT2;
  const t = 1.0 / (1.0 + p * x);
  const y = 1.0 - (((((a5 * t + a4) * t) + a3) * t + a2) * t + a1) * t * Math.exp(-x * x);

  return 0.5 * (1.0 + sign * y);
}

// ============================================================================
// SWE-Bench Task Generator (Synthetic for Demo)
// ============================================================================

function generateSyntheticSWEBenchTasks(count: number): SWEBenchTask[] {
  const tasks: SWEBenchTask[] = [];
  const repos = ['django/django', 'psf/requests', 'pallets/flask', 'numpy/numpy', 'pandas-dev/pandas'];
  const difficulties: ('easy' | 'medium' | 'hard')[] = ['easy', 'medium', 'hard'];
  const categories: ('bug_fix' | 'feature' | 'refactor' | 'test')[] = ['bug_fix', 'feature', 'refactor', 'test'];

  const issueTemplates = {
    bug_fix: [
      'Fix TypeError when calling {func} with None argument',
      'Handle edge case in {func} for empty input',
      'Fix race condition in {func} during concurrent access',
      'Resolve memory leak in {func} when processing large files',
    ],
    feature: [
      'Add support for {feature} in {module}',
      'Implement {feature} method on {class}',
      'Add optional {param} parameter to {func}',
    ],
    refactor: [
      'Refactor {func} to use generator instead of list',
      'Simplify {class} by extracting {method}',
      'Improve performance of {func} using caching',
    ],
    test: [
      'Add unit tests for {func} edge cases',
      'Improve test coverage for {module}',
      'Add integration tests for {feature}',
    ],
  };

  for (let i = 0; i < count; i++) {
    const difficulty = difficulties[i % difficulties.length];
    const category = categories[Math.floor(i / 3) % categories.length];
    const repo = repos[i % repos.length];

    const templates = issueTemplates[category];
    const template = templates[i % templates.length];
    const issue = template
      .replace('{func}', `process_${i}`)
      .replace('{module}', `module_${i % 5}`)
      .replace('{class}', `Class${i % 10}`)
      .replace('{method}', `method_${i % 3}`)
      .replace('{feature}', `feature_${i % 7}`)
      .replace('{param}', `param_${i % 4}`);

    tasks.push({
      id: `task_${i.toString().padStart(4, '0')}`,
      repo,
      issue,
      baseCommit: crypto.randomBytes(20).toString('hex'),
      testPatch: `# Test patch for task ${i}`,
      difficulty,
      category,
      expectedChanges: [`file_${i % 10}.py`],
      holdout: i >= count * 0.9, // Last 10% as holdout
    });
  }

  return tasks;
}

// ============================================================================
// Model API Clients
// ============================================================================

interface LLMClient {
  generate(prompt: string, options?: { maxTokens?: number; temperature?: number }): Promise<{
    text: string;
    usage: { inputTokens: number; outputTokens: number };
    latencyMs: number;
  }>;
}

class OpenAIClient implements LLMClient {
  private apiKey: string;
  private model: string;

  constructor(apiKey: string, model: string) {
    this.apiKey = apiKey;
    this.model = model;
  }

  async generate(prompt: string, options?: { maxTokens?: number; temperature?: number }) {
    const start = performance.now();

    const response = await fetch('https://api.openai.com/v1/chat/completions', {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${this.apiKey}`,
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        model: this.model,
        messages: [{ role: 'user', content: prompt }],
        max_tokens: options?.maxTokens || 2000,
        temperature: options?.temperature ?? 0.2, // Lower temp for code
      }),
    });

    const latencyMs = performance.now() - start;

    if (!response.ok) {
      throw new Error(`OpenAI API error: ${response.status}`);
    }

    const data = await response.json() as {
      choices: { message: { content: string } }[];
      usage?: { prompt_tokens?: number; completion_tokens?: number };
    };

    return {
      text: data.choices[0].message.content,
      usage: {
        inputTokens: data.usage?.prompt_tokens || 0,
        outputTokens: data.usage?.completion_tokens || 0,
      },
      latencyMs,
    };
  }
}

class AnthropicClient implements LLMClient {
  private apiKey: string;
  private model: string;

  constructor(apiKey: string, model: string) {
    this.apiKey = apiKey;
    this.model = model;
  }

  async generate(prompt: string, options?: { maxTokens?: number; temperature?: number }) {
    const start = performance.now();

    const response = await fetch('https://api.anthropic.com/v1/messages', {
      method: 'POST',
      headers: {
        'x-api-key': this.apiKey,
        'anthropic-version': '2023-06-01',
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        model: this.model,
        messages: [{ role: 'user', content: prompt }],
        max_tokens: options?.maxTokens || 2000,
        temperature: options?.temperature ?? 0.2,
      }),
    });

    const latencyMs = performance.now() - start;

    if (!response.ok) {
      throw new Error(`Anthropic API error: ${response.status}`);
    }

    const data = await response.json() as {
      content: { text: string }[];
      usage?: { input_tokens?: number; output_tokens?: number };
    };

    return {
      text: data.content[0].text,
      usage: {
        inputTokens: data.usage?.input_tokens || 0,
        outputTokens: data.usage?.output_tokens || 0,
      },
      latencyMs,
    };
  }
}

class OpenRouterClient implements LLMClient {
  private apiKey: string;
  private model: string;

  constructor(apiKey: string, model: string) {
    this.apiKey = apiKey;
    this.model = model;
  }

  async generate(prompt: string, options?: { maxTokens?: number; temperature?: number }) {
    const start = performance.now();

    const response = await fetch('https://openrouter.ai/api/v1/chat/completions', {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${this.apiKey}`,
        'Content-Type': 'application/json',
        'HTTP-Referer': 'https://github.com/ruvnet/ruvector',
      },
      body: JSON.stringify({
        model: this.model,
        messages: [{ role: 'user', content: prompt }],
        max_tokens: options?.maxTokens || 2000,
        temperature: options?.temperature ?? 0.2,
      }),
    });

    const latencyMs = performance.now() - start;

    if (!response.ok) {
      throw new Error(`OpenRouter API error: ${response.status}`);
    }

    const data = await response.json() as {
      choices: { message: { content: string } }[];
      usage?: { prompt_tokens?: number; completion_tokens?: number };
    };

    return {
      text: data.choices[0].message.content,
      usage: {
        inputTokens: data.usage?.prompt_tokens || 0,
        outputTokens: data.usage?.completion_tokens || 0,
      },
      latencyMs,
    };
  }
}

class MockClient implements LLMClient {
  private model: string;
  private successRate: number;

  constructor(model: string, successRate: number = 0.3) {
    this.model = model;
    this.successRate = successRate;
  }

  async generate(prompt: string, _options?: { maxTokens?: number; temperature?: number }) {
    await new Promise(r => setTimeout(r, 50 + Math.random() * 100)); // Simulate latency

    const resolved = Math.random() < this.successRate;

    return {
      text: resolved
        ? `# Generated patch for the issue\n--- a/file.py\n+++ b/file.py\n@@ -1,5 +1,7 @@\n def func():\n+    # Fix applied\n     pass`
        : `# Unable to resolve the issue completely\n# Partial analysis...`,
      usage: {
        inputTokens: Math.floor(prompt.length / 4),
        outputTokens: resolved ? 200 : 100,
      },
      latencyMs: 50 + Math.random() * 100,
    };
  }
}

// ============================================================================
// Small Model Registry (December 2025)
// ============================================================================

function getSmallModelRegistry(): SmallModelConfig[] {
  const openaiKey = process.env.OPENAI_API_KEY;
  const anthropicKey = process.env.ANTHROPIC_API_KEY;
  const openrouterKey = process.env.OPENROUTER_API_KEY;

  return [
    // OpenAI Nano/Mini Models
    {
      name: 'GPT-4.1-nano',
      provider: 'openai',
      modelId: 'gpt-4.1-nano-2025-04-14',
      apiKey: openaiKey,
      maxTokens: 16384,
      parameterCount: '~8B (estimated)',
      costPer1kTokens: { input: 0.0001, output: 0.0004 },
      description: 'OpenAI fastest/cheapest model, 9.8% on Aider polyglot',
    },
    {
      name: 'GPT-4.1-mini',
      provider: 'openai',
      modelId: 'gpt-4.1-mini-2025-04-14',
      apiKey: openaiKey,
      maxTokens: 128000,
      parameterCount: '~20B (estimated)',
      costPer1kTokens: { input: 0.0004, output: 0.0016 },
      description: 'OpenAI balanced small model, 23.6% on SWE-bench',
    },
    {
      name: 'GPT-5-nano',
      provider: 'openai',
      modelId: 'gpt-5-nano',
      apiKey: openaiKey,
      maxTokens: 32768,
      parameterCount: '~10B (estimated)',
      costPer1kTokens: { input: 0.00015, output: 0.0006 },
      description: 'Latest OpenAI nano, extremely cost-effective',
    },

    // Anthropic
    {
      name: 'Claude-3-Haiku',
      provider: 'anthropic',
      modelId: 'claude-3-haiku-20240307',
      apiKey: anthropicKey,
      maxTokens: 200000,
      parameterCount: '~20B (estimated)',
      costPer1kTokens: { input: 0.00025, output: 0.00125 },
      description: 'Anthropic fast/cheap model',
    },

    // Open Source via OpenRouter
    {
      name: 'Qwen2.5-Coder-7B',
      provider: 'openrouter',
      modelId: 'qwen/qwen-2.5-coder-7b-instruct',
      apiKey: openrouterKey,
      maxTokens: 32768,
      parameterCount: '7B',
      costPer1kTokens: { input: 0.00007, output: 0.00007 },
      description: 'Alibaba code-specialized 7B model',
    },
    {
      name: 'Qwen2.5-Coder-1.5B',
      provider: 'openrouter',
      modelId: 'qwen/qwen-2.5-coder-1.5b-instruct',
      apiKey: openrouterKey,
      maxTokens: 32768,
      parameterCount: '1.5B',
      costPer1kTokens: { input: 0.00002, output: 0.00002 },
      description: 'Ultra-tiny Qwen coder, extremely fast',
    },
    {
      name: 'DeepSeek-Coder-V2-Lite',
      provider: 'openrouter',
      modelId: 'deepseek/deepseek-coder-v2-lite-instruct',
      apiKey: openrouterKey,
      maxTokens: 32768,
      parameterCount: '16B (2.4B active)',
      costPer1kTokens: { input: 0.00014, output: 0.00028 },
      description: 'DeepSeek MoE code model, efficient',
    },
    {
      name: 'Devstral-Small',
      provider: 'openrouter',
      modelId: 'mistralai/devstral-small-2507',
      apiKey: openrouterKey,
      maxTokens: 32768,
      parameterCount: '24B',
      costPer1kTokens: { input: 0.0001, output: 0.0003 },
      description: 'Mistral top open model: 53.6% SWE-bench Verified',
    },
    {
      name: 'CodeLlama-7B',
      provider: 'openrouter',
      modelId: 'meta-llama/codellama-7b-instruct',
      apiKey: openrouterKey,
      maxTokens: 16384,
      parameterCount: '7B',
      costPer1kTokens: { input: 0.00007, output: 0.00007 },
      description: 'Meta code-specialized Llama',
    },
    {
      name: 'Phi-4',
      provider: 'openrouter',
      modelId: 'microsoft/phi-4',
      apiKey: openrouterKey,
      maxTokens: 16384,
      parameterCount: '14B',
      costPer1kTokens: { input: 0.00007, output: 0.00014 },
      description: 'Microsoft small model: 18.5% SWE-bench',
    },
    {
      name: 'StarCoder2-7B',
      provider: 'openrouter',
      modelId: 'bigcode/starcoder2-7b',
      apiKey: openrouterKey,
      maxTokens: 16384,
      parameterCount: '7B',
      costPer1kTokens: { input: 0.00007, output: 0.00007 },
      description: 'BigCode StarCoder2 7B',
    },
  ];
}

// ============================================================================
// SWE-Bench Evaluation Logic
// ============================================================================

function createSWEBenchPrompt(task: SWEBenchTask): string {
  return `You are an expert software engineer. Solve the following GitHub issue by providing a patch.

Repository: ${task.repo}
Issue: ${task.issue}
Base Commit: ${task.baseCommit}
${task.hint ? `Hint: ${task.hint}` : ''}

Expected files to modify: ${task.expectedChanges.join(', ')}

Please provide:
1. Your analysis of the issue
2. A unified diff patch that resolves the issue

Format your response as:
<analysis>
Your reasoning here
</analysis>

<patch>
--- a/file.py
+++ b/file.py
@@ -line,count +line,count @@
 context
-removed line
+added line
 context
</patch>`;
}

function evaluateResponse(response: string, task: SWEBenchTask): {
  resolved: boolean;
  partiallyResolved: boolean;
  testsPassRate: number;
} {
  // Check if patch is present
  const hasPatch = response.includes('<patch>') && response.includes('</patch>');
  const hasAnalysis = response.includes('<analysis>');
  const hasDiffFormat = response.includes('---') && response.includes('+++');

  // Simulate test execution (in production, would actually run tests)
  let testsPassRate = 0;

  if (hasPatch && hasDiffFormat) {
    // Check if expected files are mentioned
    const mentionsExpectedFiles = task.expectedChanges.some(f => response.includes(f));

    // Difficulty-based success simulation
    const difficultyMultiplier = {
      easy: 0.7,
      medium: 0.5,
      hard: 0.3,
    };

    const baseRate = mentionsExpectedFiles ? 0.6 : 0.3;
    testsPassRate = baseRate * difficultyMultiplier[task.difficulty] + Math.random() * 0.2;
  }

  return {
    resolved: testsPassRate > 0.9,
    partiallyResolved: testsPassRate > 0.5 && testsPassRate <= 0.9,
    testsPassRate,
  };
}

// ============================================================================
// Main Benchmark Runner
// ============================================================================

export class SWEBenchHarness {
  private tasks: SWEBenchTask[];
  private models: Map<string, { config: SmallModelConfig; client: LLMClient }>;
  private results: Map<string, TaskResult[]>;
  private outputDir: string;
  private kFolds: number;
  private seed: string;

  constructor(options: {
    tasks?: SWEBenchTask[];
    outputDir?: string;
    kFolds?: number;
    seed?: string;
  } = {}) {
    this.tasks = options.tasks || generateSyntheticSWEBenchTasks(100);
    this.outputDir = options.outputDir || './benchmarks/results';
    this.kFolds = options.kFolds || 5;
    this.seed = options.seed || 'swe-bench-2025-12';
    this.models = new Map();
    this.results = new Map();
  }

  /**
   * Register a model for benchmarking
   */
  addModel(config: SmallModelConfig): void {
    let client: LLMClient;

    if (!config.apiKey) {
      console.log(`âš ï¸  No API key for ${config.name}, using mock client`);
      // Simulate different success rates based on model size
      const paramNum = parseFloat(config.parameterCount) || 7;
      const mockSuccessRate = Math.min(0.6, 0.15 + (paramNum / 100));
      client = new MockClient(config.name, mockSuccessRate);
    } else {
      switch (config.provider) {
        case 'openai':
          client = new OpenAIClient(config.apiKey, config.modelId);
          break;
        case 'anthropic':
          client = new AnthropicClient(config.apiKey, config.modelId);
          break;
        case 'openrouter':
        case 'deepseek':
          client = new OpenRouterClient(config.apiKey, config.modelId);
          break;
        default:
          client = new MockClient(config.name);
      }
    }

    this.models.set(config.name, { config, client });
    console.log(`âœ“ Registered: ${config.name} (${config.parameterCount})`);
  }

  /**
   * Run benchmark for a single model on task set
   */
  async benchmarkModel(
    modelName: string,
    taskSet: SWEBenchTask[]
  ): Promise<TaskResult[]> {
    const model = this.models.get(modelName);
    if (!model) throw new Error(`Model ${modelName} not registered`);

    const results: TaskResult[] = [];

    for (const task of taskSet) {
      try {
        const prompt = createSWEBenchPrompt(task);
        const response = await model.client.generate(prompt, {
          maxTokens: model.config.maxTokens,
          temperature: model.config.temperature || 0.2,
        });

        const evaluation = evaluateResponse(response.text, task);
        const cost =
          (response.usage.inputTokens / 1000) * model.config.costPer1kTokens.input +
          (response.usage.outputTokens / 1000) * model.config.costPer1kTokens.output;

        results.push({
          taskId: task.id,
          modelName,
          resolved: evaluation.resolved,
          partiallyResolved: evaluation.partiallyResolved,
          generatedPatch: response.text,
          testsPassRate: evaluation.testsPassRate,
          latencyMs: response.latencyMs,
          tokensUsed: response.usage,
          cost,
        });
      } catch (error: any) {
        results.push({
          taskId: task.id,
          modelName,
          resolved: false,
          partiallyResolved: false,
          generatedPatch: '',
          testsPassRate: 0,
          latencyMs: 0,
          tokensUsed: { inputTokens: 0, outputTokens: 0 },
          cost: 0,
          error: error.message,
        });
      }
    }

    return results;
  }

  /**
   * Run full benchmark with anti-overfitting measures
   */
  async runBenchmark(): Promise<BenchmarkReport> {
    console.log('\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—');
    console.log('â•‘     SWE-Bench Small Model Benchmark (December 2025)            â•‘');
    console.log('â•‘     Anti-Overfitting Analysis Enabled                          â•‘');
    console.log('â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£');
    console.log(`â•‘  Total Tasks: ${this.tasks.filter(t => !t.holdout).length.toString().padEnd(46)}â•‘`);
    console.log(`â•‘  Holdout Tasks: ${this.tasks.filter(t => t.holdout).length.toString().padEnd(44)}â•‘`);
    console.log(`â•‘  Models: ${this.models.size.toString().padEnd(51)}â•‘`);
    console.log(`â•‘  K-Folds: ${this.kFolds.toString().padEnd(50)}â•‘`);
    console.log('â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n');

    await fs.mkdir(this.outputDir, { recursive: true });

    const nonHoldoutTasks = this.tasks.filter(t => !t.holdout);
    const holdoutTasks = this.tasks.filter(t => t.holdout);

    // Stratified split
    const { train, valid, test } = stratifiedSplit(
      nonHoldoutTasks, 0.6, 0.2, this.seed
    );

    console.log(`ğŸ“Š Data Split: Train=${train.length}, Valid=${valid.length}, Test=${test.length}, Holdout=${holdoutTasks.length}\n`);

    const modelMetrics: ModelBenchmarkMetrics[] = [];

    for (const [modelName, { config }] of this.models) {
      console.log(`\nğŸ”¬ Benchmarking: ${modelName} (${config.parameterCount})`);
      console.log('â”€'.repeat(60));

      // 1. Train/Valid/Test evaluation
      console.log('  â†’ Running train set...');
      const trainResults = await this.benchmarkModel(modelName, train);

      console.log('  â†’ Running validation set...');
      const validResults = await this.benchmarkModel(modelName, valid);

      console.log('  â†’ Running test set...');
      const testResults = await this.benchmarkModel(modelName, test);

      // 2. K-Fold Cross-Validation
      console.log(`  â†’ Running ${this.kFolds}-fold CV...`);
      const cvScores: number[] = [];

      for (const { fold, trainSet, testSet } of kFoldGenerator(nonHoldoutTasks, this.kFolds, this.seed)) {
        const foldResults = await this.benchmarkModel(modelName, testSet);
        const foldResolveRate = foldResults.filter(r => r.resolved).length / foldResults.length;
        cvScores.push(foldResolveRate);
        console.log(`    Fold ${fold}: ${(foldResolveRate * 100).toFixed(1)}%`);
      }

      // Calculate metrics
      const allResults = [...trainResults, ...validResults, ...testResults];
      const trainResolveRate = trainResults.filter(r => r.resolved).length / trainResults.length;
      const validResolveRate = validResults.filter(r => r.resolved).length / validResults.length;
      const testResolveRate = testResults.filter(r => r.resolved).length / testResults.length;

      const cvMean = cvScores.reduce((a, b) => a + b, 0) / cvScores.length;
      const cvStd = Math.sqrt(cvScores.reduce((s, x) => s + (x - cvMean) ** 2, 0) / cvScores.length);
      const cvCI = bootstrapCI(cvScores);

      const latencies = allResults.map(r => r.latencyMs).filter(l => l > 0);
      const totalCost = allResults.reduce((s, r) => s + r.cost, 0);
      const resolvedCount = allResults.filter(r => r.resolved).length;

      latencies.sort((a, b) => a - b);

      const metrics: ModelBenchmarkMetrics = {
        modelName,
        parameterCount: config.parameterCount,

        resolveRate: allResults.filter(r => r.resolved).length / allResults.length,
        partialResolveRate: allResults.filter(r => r.partiallyResolved).length / allResults.length,
        testPassRate: allResults.reduce((s, r) => s + r.testsPassRate, 0) / allResults.length,

        trainResolveRate,
        validationResolveRate: validResolveRate,
        testResolveRate,
        overfitGap: trainResolveRate - testResolveRate,

        cvMean,
        cvStd,
        cvConfidenceInterval: cvCI,

        avgLatencyMs: latencies.reduce((a, b) => a + b, 0) / latencies.length,
        p95LatencyMs: latencies[Math.floor(latencies.length * 0.95)] || 0,
        throughputTasksPerMin: 60000 / (latencies.reduce((a, b) => a + b, 0) / latencies.length),

        totalCost,
        costPerResolvedTask: resolvedCount > 0 ? totalCost / resolvedCount : Infinity,
        costEfficiencyScore: resolvedCount > 0
          ? (allResults.filter(r => r.resolved).length / allResults.length) / totalCost
          : 0,

        statisticallyBetterThan: [],
      };

      modelMetrics.push(metrics);
      this.results.set(modelName, allResults);

      console.log(`  âœ“ Test Resolve Rate: ${(testResolveRate * 100).toFixed(1)}%`);
      console.log(`  âœ“ CV Mean Â± Std: ${(cvMean * 100).toFixed(1)}% Â± ${(cvStd * 100).toFixed(1)}%`);
      console.log(`  âœ“ Overfit Gap: ${(metrics.overfitGap * 100).toFixed(1)}%`);
      console.log(`  âœ“ Cost/Resolved: $${metrics.costPerResolvedTask.toFixed(4)}`);
    }

    // Statistical significance testing
    console.log('\nğŸ“ˆ Computing statistical significance...');
    for (let i = 0; i < modelMetrics.length; i++) {
      const modelAResults = this.results.get(modelMetrics[i].modelName)!;
      const modelAScores = modelAResults.map(r => r.resolved ? 1 : 0);

      for (let j = 0; j < modelMetrics.length; j++) {
        if (i === j) continue;

        const modelBResults = this.results.get(modelMetrics[j].modelName)!;
        const modelBScores = modelBResults.map(r => r.resolved ? 1 : 0);

        const { significant, pValue } = tTest(modelAScores, modelBScores);

        if (significant && modelMetrics[i].cvMean > modelMetrics[j].cvMean) {
          modelMetrics[i].statisticallyBetterThan.push(modelMetrics[j].modelName);
        }
      }
    }

    // Generate report
    const report = this.generateReport(modelMetrics);

    // Save results
    const timestamp = new Date().toISOString().replace(/[:.]/g, '-');
    await fs.writeFile(
      path.join(this.outputDir, `swe-bench-report-${timestamp}.json`),
      JSON.stringify(report, null, 2)
    );

    await this.printReport(report);

    return report;
  }

  /**
   * Generate comprehensive report
   */
  private generateReport(modelMetrics: ModelBenchmarkMetrics[]): BenchmarkReport {
    // Sort for rankings
    const byResolve = [...modelMetrics].sort((a, b) => b.cvMean - a.cvMean);
    const byCostEff = [...modelMetrics].sort((a, b) => b.costEfficiencyScore - a.costEfficiencyScore);
    const byLatency = [...modelMetrics].sort((a, b) => a.avgLatencyMs - b.avgLatencyMs);
    const byOverfit = [...modelMetrics].sort((a, b) => Math.abs(a.overfitGap) - Math.abs(b.overfitGap));

    // Overall ranking (weighted score)
    const overallScores = modelMetrics.map(m => ({
      name: m.modelName,
      score:
        m.cvMean * 0.40 +                           // Quality weight
        (1 / (m.costPerResolvedTask + 0.001)) * 0.25 + // Cost efficiency
        (1 / (m.avgLatencyMs + 1)) * 1000 * 0.15 +  // Speed
        (1 - Math.abs(m.overfitGap)) * 0.20,        // Generalization
    }));
    overallScores.sort((a, b) => b.score - a.score);

    const best = byResolve[0];

    return {
      timestamp: new Date().toISOString(),
      totalTasks: this.tasks.filter(t => !t.holdout).length,
      totalModels: this.models.size,
      kFolds: this.kFolds,

      rankings: {
        byResolveRate: byResolve.map(m => m.modelName),
        byCostEfficiency: byCostEff.map(m => m.modelName),
        byLatency: byLatency.map(m => m.modelName),
        byOverfitResistance: byOverfit.map(m => m.modelName),
        overall: overallScores.map(s => s.name),
      },

      modelMetrics,

      bestSmallModel: {
        name: best.modelName,
        reason: `Highest CV resolve rate (${(best.cvMean * 100).toFixed(1)}%) with 95% CI [${(best.cvConfidenceInterval[0] * 100).toFixed(1)}%, ${(best.cvConfidenceInterval[1] * 100).toFixed(1)}%]`,
        metrics: best,
      },

      antiOverfittingAnalysis: {
        averageOverfitGap: modelMetrics.reduce((s, m) => s + Math.abs(m.overfitGap), 0) / modelMetrics.length,
        modelsWithSignificantOverfit: modelMetrics
          .filter(m => m.overfitGap > 0.1)
          .map(m => m.modelName),
        recommendations: this.generateRecommendations(modelMetrics),
      },
    };
  }

  /**
   * Generate recommendations based on analysis
   */
  private generateRecommendations(metrics: ModelBenchmarkMetrics[]): string[] {
    const recs: string[] = [];

    const avgOverfit = metrics.reduce((s, m) => s + m.overfitGap, 0) / metrics.length;
    if (avgOverfit > 0.05) {
      recs.push('Average overfit gap is high. Consider more regularization or data augmentation.');
    }

    const bestCostEff = metrics.reduce((a, b) => a.costEfficiencyScore > b.costEfficiencyScore ? a : b);
    const bestQuality = metrics.reduce((a, b) => a.cvMean > b.cvMean ? a : b);

    if (bestCostEff.modelName !== bestQuality.modelName) {
      recs.push(`For cost-sensitive deployments, use ${bestCostEff.modelName}. For quality-critical tasks, use ${bestQuality.modelName}.`);
    }

    const tinyModels = metrics.filter(m => {
      const params = parseFloat(m.parameterCount);
      return !isNaN(params) && params < 10;
    });

    if (tinyModels.length > 0) {
      const bestTiny = tinyModels.reduce((a, b) => a.cvMean > b.cvMean ? a : b);
      recs.push(`Best sub-10B model: ${bestTiny.modelName} with ${(bestTiny.cvMean * 100).toFixed(1)}% resolve rate.`);
    }

    return recs;
  }

  /**
   * Print formatted report
   */
  private async printReport(report: BenchmarkReport): Promise<void> {
    console.log('\n');
    console.log('â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—');
    console.log('â•‘                    SWE-BENCH SMALL MODEL BENCHMARK RESULTS                         â•‘');
    console.log('â•‘                         December 2025 - Anti-Overfit Analysis                      â•‘');
    console.log('â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£');

    console.log('â•‘                                                                                    â•‘');
    console.log('â•‘  ğŸ† OVERALL RANKINGS (Weighted: Quality 40%, Cost 25%, Speed 15%, Generalization 20%) â•‘');
    console.log('â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â•‘');

    for (let i = 0; i < Math.min(5, report.rankings.overall.length); i++) {
      const name = report.rankings.overall[i];
      const m = report.modelMetrics.find(m => m.modelName === name)!;
      const medal = i === 0 ? 'ğŸ¥‡' : i === 1 ? 'ğŸ¥ˆ' : i === 2 ? 'ğŸ¥‰' : '  ';
      console.log(`â•‘  ${medal} ${(i + 1)}. ${name.padEnd(25)} CV: ${(m.cvMean * 100).toFixed(1).padStart(5)}% Â± ${(m.cvStd * 100).toFixed(1).padStart(4)}% â”‚ Gap: ${(m.overfitGap * 100).toFixed(1).padStart(5)}% â•‘`);
    }

    console.log('â•‘                                                                                    â•‘');
    console.log('â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£');
    console.log('â•‘                              DETAILED MODEL METRICS                                â•‘');
    console.log('â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£');
    console.log('â•‘ Model                    â”‚ Params â”‚ Test %  â”‚ CV Mean â”‚ CV 95% CI     â”‚ Overfit   â•‘');
    console.log('â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£');

    for (const m of report.modelMetrics.sort((a, b) => b.cvMean - a.cvMean)) {
      const ciStr = `[${(m.cvConfidenceInterval[0] * 100).toFixed(0)}%-${(m.cvConfidenceInterval[1] * 100).toFixed(0)}%]`;
      console.log(`â•‘ ${m.modelName.padEnd(24)} â”‚ ${m.parameterCount.padEnd(6)} â”‚ ${(m.testResolveRate * 100).toFixed(1).padStart(6)}% â”‚ ${(m.cvMean * 100).toFixed(1).padStart(6)}% â”‚ ${ciStr.padEnd(13)} â”‚ ${(m.overfitGap * 100).toFixed(1).padStart(5)}%   â•‘`);
    }

    console.log('â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£');
    console.log('â•‘                              COST ANALYSIS                                         â•‘');
    console.log('â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£');
    console.log('â•‘ Model                    â”‚ Total Cost â”‚ $/Resolved â”‚ Cost Efficiency Score         â•‘');
    console.log('â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£');

    for (const m of report.modelMetrics.sort((a, b) => b.costEfficiencyScore - a.costEfficiencyScore)) {
      console.log(`â•‘ ${m.modelName.padEnd(24)} â”‚ $${m.totalCost.toFixed(4).padStart(8)} â”‚ $${m.costPerResolvedTask.toFixed(4).padStart(8)} â”‚ ${m.costEfficiencyScore.toFixed(4).padStart(29)} â•‘`);
    }

    console.log('â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£');
    console.log('â•‘                         ANTI-OVERFITTING ANALYSIS                                  â•‘');
    console.log('â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£');
    console.log(`â•‘  Average Overfit Gap: ${(report.antiOverfittingAnalysis.averageOverfitGap * 100).toFixed(2)}%`.padEnd(85) + 'â•‘');

    if (report.antiOverfittingAnalysis.modelsWithSignificantOverfit.length > 0) {
      console.log(`â•‘  âš ï¸  Models with significant overfitting (>10% gap):`.padEnd(85) + 'â•‘');
      for (const model of report.antiOverfittingAnalysis.modelsWithSignificantOverfit) {
        console.log(`â•‘     - ${model}`.padEnd(85) + 'â•‘');
      }
    } else {
      console.log(`â•‘  âœ“ No models show significant overfitting`.padEnd(85) + 'â•‘');
    }

    console.log('â•‘                                                                                    â•‘');
    console.log('â•‘  ğŸ“‹ Recommendations:'.padEnd(85) + 'â•‘');
    for (const rec of report.antiOverfittingAnalysis.recommendations) {
      const lines = this.wrapText(rec, 78);
      for (const line of lines) {
        console.log(`â•‘     ${line}`.padEnd(85) + 'â•‘');
      }
    }

    console.log('â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£');
    console.log('â•‘                              ğŸ† BEST SMALL MODEL                                   â•‘');
    console.log('â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£');
    console.log(`â•‘  Winner: ${report.bestSmallModel.name}`.padEnd(85) + 'â•‘');
    const reasonLines = this.wrapText(report.bestSmallModel.reason, 75);
    for (const line of reasonLines) {
      console.log(`â•‘  Reason: ${line}`.padEnd(85) + 'â•‘');
    }
    console.log('â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');
  }

  private wrapText(text: string, maxLen: number): string[] {
    const words = text.split(' ');
    const lines: string[] = [];
    let current = '';

    for (const word of words) {
      if ((current + ' ' + word).trim().length <= maxLen) {
        current = (current + ' ' + word).trim();
      } else {
        if (current) lines.push(current);
        current = word;
      }
    }
    if (current) lines.push(current);

    return lines;
  }
}

// ============================================================================
// CLI Runner
// ============================================================================

async function main() {
  console.log('ğŸš€ SWE-Bench Small Model Benchmark Harness');
  console.log('   @ruvector/ruvllm - December 2025\n');

  const harness = new SWEBenchHarness({
    tasks: generateSyntheticSWEBenchTasks(100),
    kFolds: 5,
    seed: 'reproducible-benchmark-2025',
  });

  // Register all small models
  const registry = getSmallModelRegistry();
  for (const config of registry) {
    harness.addModel(config);
  }

  console.log(`\nğŸ“‹ Registered ${registry.length} models for benchmarking\n`);

  try {
    const report = await harness.runBenchmark();

    console.log('\nâœ… Benchmark completed successfully!');
    console.log(`ğŸ“Š Results saved to: ./benchmarks/results/`);

    // Print final recommendation
    console.log('\n' + 'â•'.repeat(80));
    console.log(`\nğŸ¯ FINAL RECOMMENDATION for December 2025:\n`);
    console.log(`   Best Overall Small Model: ${report.bestSmallModel.name}`);
    console.log(`   CV Resolve Rate: ${(report.bestSmallModel.metrics.cvMean * 100).toFixed(1)}%`);
    console.log(`   Overfit Gap: ${(report.bestSmallModel.metrics.overfitGap * 100).toFixed(1)}%`);
    console.log(`   Cost/Resolved: $${report.bestSmallModel.metrics.costPerResolvedTask.toFixed(4)}`);
    console.log('\n' + 'â•'.repeat(80));

  } catch (error) {
    console.error('\nâŒ Benchmark failed:', error);
    process.exit(1);
  }
}

// Run if executed directly
if (require.main === module) {
  main().catch(console.error);
}

export {
  SWEBenchTask,
  SmallModelConfig,
  ModelBenchmarkMetrics,
  BenchmarkReport,
  generateSyntheticSWEBenchTasks,
  getSmallModelRegistry,
};
